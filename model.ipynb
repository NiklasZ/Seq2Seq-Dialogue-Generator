{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model setup (common to training/evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import data_utils\n",
    "import seq2seq_wrapper\n",
    "import utils\n",
    "import batcher\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "emb_dim = 1024\n",
    "N_EPOCHS = 10\n",
    "SAVE_EVERY_N_BATCHES = 100\n",
    "EVAL_EVERY_N_BATCHES = 10\n",
    "CKPT_DIR = '/var/tmp/archived_checkpoints/30914f8'\n",
    "LOGS_DIR = 'summaries'\n",
    "MODEL_NAME = 'baseline'\n",
    "\n",
    "ckpt_path = os.path.join(CKPT_DIR, 'model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/movie_script/metadata.pkl', 'rb') as f:\n",
    "    metadata = pickle.load(f)\n",
    "trainX = np.load('data/movie_script/training_q.npy')\n",
    "trainY = np.load('data/movie_script/training_a.npy')\n",
    "validX = np.load('data/movie_script/validation_q.npy')\n",
    "validY = np.load('data/movie_script/validation_a.npy')\n",
    "\n",
    "train_batch_gen = data_utils.rand_batch_gen(trainX, trainY, batch_size)\n",
    "val_batch_gen = data_utils.rand_batch_gen(validX, validY, 256)\n",
    "\n",
    "p_r = np.load('data/p_r.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how you doin ' , pal ?\n",
      "i ' m okay , sir .\n",
      "calm down . you brought it up --\n",
      "i did not , <person> .\n"
     ]
    }
   ],
   "source": [
    "for b in [train_batch_gen, val_batch_gen]:\n",
    "    q, a = b.__next__()\n",
    "    q = data_utils.decode(sequence=list(q[:, 0]), lookup=metadata['idx2w'], separator=' ')\n",
    "    a = data_utils.decode(sequence=list(a[:, 0]), lookup=metadata['idx2w'], separator=' ')\n",
    "    print(q)\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parameters \n",
    "xseq_len = trainX.shape[-1]\n",
    "yseq_len = trainY.shape[-1]\n",
    "xvocab_size = len(metadata['idx2w'])  \n",
    "yvocab_size = xvocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<log> Building Graph </log>"
     ]
    }
   ],
   "source": [
    "xseq_len = trainX.shape[-1]\n",
    "yseq_len = trainY.shape[-1]\n",
    "vocab_size = len(metadata['idx2w'])\n",
    "\n",
    "model = seq2seq_wrapper.Seq2Seq(xseq_len=xseq_len,\n",
    "                               yseq_len=yseq_len,\n",
    "                               xvocab_size=xvocab_size,\n",
    "                               yvocab_size=yvocab_size,\n",
    "                               ckpt_path='ckpt/',\n",
    "                               emb_dim=emb_dim,\n",
    "                               lstm_dim=emb_dim,\n",
    "                               num_layers=3,\n",
    "                               attention=False\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loss_summary = tf.summary.scalar(\"train_loss\", model.loss)\n",
    "validation_loss_summary = tf.summary.scalar(\"validation_loss\", model.loss)\n",
    "\n",
    "timestamp = int(time.time())                                                \n",
    "run_log_dir = os.path.join(LOGS_DIR, MODEL_NAME + '_' + str(timestamp))                        \n",
    "os.makedirs(run_log_dir)                                                    \n",
    "# (this step also writes the graph to the events file so that               \n",
    "# it shows up in TensorBoard)                                               \n",
    "summary_writer = tf.summary.FileWriter(run_log_dir, sess.graph, flush_secs=5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq2seq_wrapper.Seq2Seq.load_embeddings(sess, model, metadata, '/scratch/GoogleNews-vectors-negative300.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '.', 'really']\n",
      "['<GO>', 'this', 'is', 'going', 'to', 'be', 'so', 'good', 'for', 'you', '.', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n"
     ]
    }
   ],
   "source": [
    "x, y = train_batcher.next_batch()\n",
    "feed_dict = model.get_feed(x, y)\n",
    "enc_ip, labels = sess.run([model.enc_ip, model.labels], feed_dict)\n",
    "enc_ip = np.array(enc_ip)\n",
    "labels = np.array(labels)\n",
    "# both enc_ip and labels are sequence_len x batch_size\n",
    "print(data_utils.ids_to_words(enc_ip[:, 0], metadata['idx2w']))\n",
    "print(data_utils.ids_to_words(labels[:, 0], metadata['idx2w']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialise these in a separate cell so that we can\n",
    "# interrupt and resume the training loop\n",
    "epoch_n = 1\n",
    "batch_n = 1\n",
    "train_batcher.reset()\n",
    "validation_batcher.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feed_dict = model.get_feed(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10\n",
      "Batch 1556/2332\n",
      "Batch loss: 0.105\n",
      "Batch took 1.70 seconds\n",
      "Batch 1557/2332\n",
      "Batch loss: 0.084\n",
      "Batch took 1.63 seconds\n",
      "Batch 1558/2332\n",
      "Batch loss: 0.102\n",
      "Batch took 1.55 seconds\n",
      "Batch 1559/2332\n",
      "Batch loss: 0.088\n",
      "Batch took 1.53 seconds\n",
      "Batch 1560/2332\n",
      "Batch loss: 0.115\n",
      "Batch took 1.68 seconds\n",
      "Validation loss: 0.100\n",
      "Batch 1561/2332\n",
      "Batch loss: 0.097\n",
      "Batch took 1.69 seconds\n",
      "Batch 1562/2332\n",
      "Batch loss: 0.117\n",
      "Batch took 1.56 seconds\n",
      "Batch 1563/2332\n",
      "Batch loss: 0.096\n",
      "Batch took 1.76 seconds\n",
      "Batch 1564/2332\n",
      "Batch loss: 0.102\n",
      "Batch took 1.74 seconds\n",
      "Batch 1565/2332\n",
      "Batch loss: 0.109\n",
      "Batch took 1.54 seconds\n",
      "Batch 1566/2332\n",
      "Batch loss: 0.096\n",
      "Batch took 1.72 seconds\n",
      "Batch 1567/2332\n",
      "Batch loss: 0.105\n",
      "Batch took 1.66 seconds\n",
      "Batch 1568/2332\n",
      "Batch loss: 0.072\n",
      "Batch took 1.66 seconds\n",
      "Batch 1569/2332\n",
      "Batch loss: 0.099\n",
      "Batch took 1.64 seconds\n",
      "Batch 1570/2332\n",
      "Batch loss: 0.088\n",
      "Batch took 1.67 seconds\n",
      "Validation loss: 0.128\n",
      "Batch 1571/2332\n",
      "Batch loss: 0.077\n",
      "Batch took 1.57 seconds\n",
      "Batch 1572/2332\n",
      "Batch loss: 0.091\n",
      "Batch took 1.53 seconds\n",
      "Batch 1573/2332\n",
      "Batch loss: 0.077\n",
      "Batch took 1.75 seconds\n",
      "Batch 1574/2332\n",
      "Batch loss: 0.073\n",
      "Batch took 1.77 seconds\n",
      "Batch 1575/2332\n",
      "Batch loss: 0.074\n",
      "Batch took 1.73 seconds\n",
      "Batch 1576/2332\n",
      "Batch loss: 0.074\n",
      "Batch took 1.65 seconds\n",
      "Batch 1577/2332\n",
      "Batch loss: 0.083\n",
      "Batch took 1.69 seconds\n",
      "Batch 1578/2332\n",
      "Batch loss: 0.107\n",
      "Batch took 1.55 seconds\n",
      "Batch 1579/2332\n",
      "Batch loss: 0.075\n",
      "Batch took 1.58 seconds\n",
      "Batch 1580/2332\n",
      "Batch loss: 0.071\n",
      "Batch took 1.75 seconds\n",
      "Validation loss: 0.107\n",
      "Batch 1581/2332\n",
      "Batch loss: 0.075\n",
      "Batch took 1.56 seconds\n",
      "Batch 1582/2332\n",
      "Interrupted\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    while epoch_n <= N_EPOCHS:\n",
    "        print(\"Epoch %d/%d\" % (epoch_n, N_EPOCHS))\n",
    "        while not train_batcher.batches_finished:\n",
    "            step = (epoch_n - 1) * train_batcher.n_batches + batch_n\n",
    "            \n",
    "            start = time.time()\n",
    "            print(\"Batch %d/%d\" % (batch_n, train_batcher.n_batches))\n",
    "            x, y = train_batcher.next_batch()\n",
    "            feed_dict = model.get_feed(x, y)\n",
    "            ops = [model.train_op, model.loss, train_loss_summary]\n",
    "            _, loss_v, train_loss_summary_v = sess.run(ops, feed_dict)\n",
    "            print(\"Batch loss: %.3f\" % loss_v)\n",
    "            summary_writer.add_summary(train_loss_summary_v, step)\n",
    "            end = time.time()\n",
    "            print(\"Batch took %.2f seconds\" % (end - start))\n",
    "            \n",
    "            if batch_n % EVAL_EVERY_N_BATCHES == 0:\n",
    "                val_x, val_y = validation_batcher.next_batch()\n",
    "                if validation_batcher.batches_finished:\n",
    "                    validation_batcher.reset()\n",
    "                feed_dict = model.get_feed(val_x, val_y)\n",
    "                ops = [model.loss, validation_loss_summary]\n",
    "                loss_v, validation_loss_summary_v = sess.run(ops, feed_dict)\n",
    "                print(\"Validation loss: %.3f\" % loss_v)\n",
    "                summary_writer.add_summary(validation_loss_summary_v, step)\n",
    "                \n",
    "            if batch_n % SAVE_EVERY_N_BATCHES == 0:\n",
    "                print(\"Saving checkpoint after %d steps...\" % step)\n",
    "                saver.save(sess, ckpt_path, global_step=step)\n",
    "            batch_n += 1\n",
    "        train_batcher.reset()\n",
    "        batch_n = 1\n",
    "        epoch_n += 1\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print('Interrupted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring checkpoint from /var/tmp/archived_checkpoints/30914f8/baseline.ckpt-50000...\n"
     ]
    }
   ],
   "source": [
    "checkpoint_file = tf.train.latest_checkpoint(CKPT_DIR)\n",
    "print(\"Restoring checkpoint from %s...\" % checkpoint_file)\n",
    "saver.restore(sess, checkpoint_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get 1000 samples from the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valX = validX[:1000,:]\n",
    "valY = validY[:1000,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set p_r (used for diversity-enhanced model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.p_r = p_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valY_pred = model.predict(sess, valX.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q : [you wish !]; a : [<person> , i ' m sorry .]\n",
      "q : [they want me to fly back tonight .]; a : [you ' re not going to be a good time ?]\n",
      "q : [<person> ? !]; a : [<person> , <person> .]\n",
      "q : [the mental hospital .]; a : [the same time .]\n",
      "q : [absolutely not . no . that is not going on now .]; a : [i ' m not going to be a little bit .]\n",
      "q : [oh my god , you already did it . was it amazing ?]; a : [i don ' t know .]\n",
      "q : [yeah .]; a : [<person> ' t you know ?]\n",
      "q : [<unk> . they have their own label that ' s just outstanding .]; a : [i ' m sorry .]\n",
      "q : [wan na play ?]; a : [no , no .]\n",
      "q : [<person> shit . <person> was two years ago ? i guess so . she was <unk> hot , though .]; a : [<person> ' s been a little bit .]\n",
      "q : [what ?]; a : [i ' m going to be a little bit .]\n",
      "q : [that ' s bullshit .]; a : [no , i ' m not .]\n",
      "q : [yeah . i just thought this might change things . i hoped . ugh .]; a : [<person> ? what ' s the matter ?]\n",
      "q : [i was afraid .]; a : [you ' re not going to be a good time .]\n",
      "q : [i wish to . go back to the way i was .]; a : [you ' re not going to be a little time ?]\n",
      "q : [end ?]; a : [yes . <person> .]\n",
      "q : [i ' d like to . <continued_utterance> would you put some of your singing in this for the baby ?]; a : [no . i ' m not going to be a little bit .]\n",
      "q : [no .]; a : [<person> ' t you like me ?]\n",
      "q : [come on guy , you know i only rib you because i love you so much !]; a : [<person> , i ' m not going to be a little bit .]\n",
      "q : [they were <unk> boys , yes sir .]; a : [and what ' s the matter ?]\n",
      "q : [where can i find him . ?]; a : [the guy .]\n",
      "q : [no , sir .]; a : [<person> ' t you like to be a little time ?]\n",
      "q : [the admiral ' s son .]; a : [<person> ' s go .]\n",
      "q : [what would that be ?]; a : [the car .]\n",
      "q : [if you didn ' t know him -- then who did ?]; a : [he ' s not a little one .]\n",
      "q : [. sir .]; a : [<person> ?]\n",
      "q : [yes you do . be honest .]; a : [i ' m not sure .]\n",
      "q : [give him the hammer , ted .]; a : [<person> , <person> !]\n",
      "q : [this was a crummy summer .]; a : [<person> , <person> ?]\n",
      "q : [-- only my prom date wasn ' t packing a gun .]; a : [<person> ' t you know that ?]\n",
      "q : [girl ?]; a : [<person> . <person> .]\n",
      "q : [how much skin and <unk> contact ?]; a : [two years ago .]\n",
      "q : [oh , i don ' t know . maybe . he ' s very <unk> .]; a : [he ' s not a good man .]\n",
      "q : [but he saw you , right ?]; a : [no , i don ' t know .]\n",
      "q : [-- yes .]; a : [and you ' re not going to be a little time ?]\n",
      "q : [<person> ' ll think so , too .]; a : [<person> ' t you know what i ' m talking about ?]\n",
      "q : [and what precisely am i supposed to know ?]; a : [you ' re not going to be a little bit of the world .]\n",
      "q : [just <unk> tonight . <person> been in ?]; a : [yeah . <person> .]\n",
      "q : [there was an accident . drunk driving . they weren ' t sure .]; a : [i ' m not going to be a little time .]\n",
      "q : [<person> , but this time i got the money .]; a : [you ' re not going to be a little bit of the world ?]\n",
      "q : [<unk> going toward <unk> ? . what sort of place is it . ?]; a : [you ' re not going to be a little bit .]\n",
      "q : [what does it say ?]; a : [it ' s a lot of people .]\n",
      "q : [yes . you ' re a great help to us .]; a : [i ' m not .]\n",
      "q : [mr . <unk> is shooting . you got an appointment ?]; a : [<person> ' s <person> .]\n",
      "q : [and you freaked . a little .]; a : [<person> ' s a little time .]\n",
      "q : [over six feet ?]; a : [you ' re a little one .]\n",
      "q : [has he ?]; a : [he ' s a little one .]\n",
      "q : [you got those for me on <person> ' s day . when there were still <unk> around here .]; a : [<person> ' s not a good idea .]\n",
      "q : [and , instead , he printed his <unk> in your face ?]; a : [yes , sir .]\n",
      "q : [ah , ah , ah .]; a : [<person> ' s sake .]\n",
      "q : [is this . <person> ?]; a : [yes , <person> .]\n",
      "q : [hello .]; a : [<person> .]\n",
      "q : [it <unk> him . most serial killers keep some sort of <unk> .]; a : [<person> ' s a little time ?]\n",
      "53\n"
     ]
    }
   ],
   "source": [
    "replies = []\n",
    "for ii, oi in zip(valX, valY_pred):\n",
    "    q = data_utils.decode(sequence=ii, lookup=metadata['idx2w'], separator=' ')\n",
    "    decoded = data_utils.decode(sequence=oi, lookup=metadata['idx2w'], separator=' ').split(' ')\n",
    "    if decoded not in replies:\n",
    "        print('q : [{0}]; a : [{1}]'.format(q, ' '.join(decoded)))\n",
    "        replies.append(decoded)\n",
    "print(len(replies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('baseline_1000_answers.txt', 'w') as file:\n",
    "    for ii, oi in zip(valX, valY_pred):\n",
    "        q = data_utils.decode(sequence=ii, lookup=metadata['idx2w'], separator=' ')\n",
    "        decoded = data_utils.decode(sequence=oi, lookup=metadata['idx2w'], separator=' ').split(' ')\n",
    "        file.write('q : [{0}]; a : [{1}]'.format(q, ' '.join(decoded)) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/niklas_zwingenberger/nlu/Baseline/seq2seq_wrapper.py:305: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "  if Y == None:\n",
      "/home/niklas_zwingenberger/nlu/Baseline/utils.py:69: RuntimeWarning: divide by zero encountered in log2\n",
      "  logp = np.log2(word_probabilities)[reference[i,:] != pad_id]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 20 is out of bounds for axis 0 with size 20",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-a290c7771a75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mword_probabilities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msentence_perplexities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperplexity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_probabilities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreference\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/niklas_zwingenberger/nlu/Baseline/utils.py\u001b[0m in \u001b[0;36mperplexity\u001b[0;34m(sentences, reference, debug, metadata)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mword_probabilities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreference\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0mword_probabilities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreference\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreference\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mpad_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mperp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 20 is out of bounds for axis 0 with size 20"
     ]
    }
   ],
   "source": [
    "logits = model.predict(sess, valX.T, Y=valY.T, argmax=False)\n",
    "word_probabilities = utils.softmax(logits, axis=-1)\n",
    "sentence_perplexities = utils.perplexity(word_probabilities, reference=valY.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 20, 10002)\n",
      "(1000, 20)\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(word_probabilities.shape)\n",
    "print(valY.shape)\n",
    "print(len(sentence_perplexities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence_perplexities = utils.perplexity(word_probabilities, reference=valY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[48.31062117331394, 835.75975407514022, 13.987229119865045, 32.334449339249893, 109.80054101722652]\n"
     ]
    }
   ],
   "source": [
    "# print a selection of 5\n",
    "print(sentence_perplexities[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('baseline_1000_perplexity.txt', 'w') as file:\n",
    "    for perp in sentence_perplexities:\n",
    "        file.write(str(perp)+ '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate BLEU score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "replies = []\n",
    "for ii, oi in zip(valX, valY_pred):\n",
    "    q = data_utils.decode(sequence=ii, lookup=metadata['idx2w'], separator=' ')\n",
    "    decoded = data_utils.decode(sequence=oi, lookup=metadata['idx2w'], separator=' ').split(' ')\n",
    "    replies.append(decoded)\n",
    "print(len(replies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/niklas_zwingenberger/.local/lib/python3.5/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "bleu_score = []\n",
    "for reference, reply in zip(valY, replies):\n",
    "    reference = data_utils.decode(reference, metadata['idx2w'])\n",
    "    bleu_score.append(nltk.translate.bleu_score.sentence_bleu(reference, reply))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(len(bleu_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('baseline_1000_bleu.txt', 'w') as file:\n",
    "    for bleu in bleu_score:\n",
    "        file.write(str(bleu)+ '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate 'max branching' score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_branch_scores = utils.max_branching_score(valY_pred.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('baseline_1000_branch.txt', 'w') as file:\n",
    "    for branch in max_branch_scores:\n",
    "        file.write(str(branch)+ '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate p_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_r = np.full(xvocab_size, 1e-12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ...using saved answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentences = []\n",
    "sentences_in_words = []\n",
    "with open('attention_1000_answers.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        sentence = []\n",
    "        sentences_in_words.append(((line.split('[')[-1]).split(']')[0]))\n",
    "        for word in ((line.split('[')[-1]).split(']')[0]).split(' ')[:-1]:\n",
    "            try:sentence.append(metadata['w2idx'][word])\n",
    "            except: pass\n",
    "        sentences.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for sentence in sentences:\n",
    "    for word_id in sentence:\n",
    "        p_r[word_id] += 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ...using generated answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(valY_pred.shape[0]):\n",
    "    for j in range(valY_pred.shape[1]):\n",
    "        word_id = valY_pred[i,j]\n",
    "        if word_id != 0:\n",
    "            p_r[word_id] += 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalise and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_r = p_r / np.sum(p_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('p_r.npy', p_r)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
